apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: vm-proxy-auth-raft
  namespace: monitoring
  labels:
    app: vm-proxy-auth
    component: raft-cluster
spec:
  serviceName: vm-proxy-auth-raft-headless
  replicas: 3
  selector:
    matchLabels:
      app: vm-proxy-auth
      component: raft-cluster
  template:
    metadata:
      labels:
        app: vm-proxy-auth
        component: raft-cluster
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: vm-proxy-auth
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      initContainers:
      - name: config-setup
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          # Generate node-specific configuration
          NODE_ID="node-$(echo $HOSTNAME | sed 's/.*-//')"
          RAFT_BIND_ADDRESS="0.0.0.0:9000"
          RAFT_DATA_DIR="/data/raft"
          
          # Create data directory
          mkdir -p /data/raft
          
          # Set environment variables for main container
          echo "NODE_ID=$NODE_ID" > /shared/env
          echo "RAFT_BIND_ADDRESS=$RAFT_BIND_ADDRESS" >> /shared/env
          echo "RAFT_DATA_DIR=$RAFT_DATA_DIR" >> /shared/env
          
          echo "Initialized configuration for $NODE_ID"
        volumeMounts:
        - name: shared-config
          mountPath: /shared
        - name: data
          mountPath: /data
      containers:
      - name: vm-proxy-auth
        image: vm-proxy-auth:latest
        imagePullPolicy: IfNotPresent
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        - name: raft
          containerPort: 9000
          protocol: TCP
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: SERVER_ADDRESS
          value: "0.0.0.0:8080"
        - name: LOG_LEVEL
          value: "info"
        - name: KUBECONFIG
          value: ""  # Use in-cluster config
        envFrom:
        - configMapRef:
            name: vm-proxy-auth-config
            optional: true
        command:
        - sh
        - -c
        - |
          # Source node-specific environment
          source /shared/env
          echo "Starting vm-proxy-auth with NODE_ID=$NODE_ID"
          
          # Start the application
          exec /usr/local/bin/vm-proxy-auth --config /etc/config/config.yaml
        volumeMounts:
        - name: config
          mountPath: /etc/config
          readOnly: true
        - name: data
          mountPath: /data
        - name: shared-config
          mountPath: /shared
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 15
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 1
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
            ephemeral-storage: 1Gi
          limits:
            cpu: 500m
            memory: 512Mi
            ephemeral-storage: 5Gi
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      volumes:
      - name: config
        configMap:
          name: vm-proxy-auth-raft-config
      - name: shared-config
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd  # Use appropriate storage class
      resources:
        requests:
          storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: vm-proxy-auth-raft-headless
  namespace: monitoring
  labels:
    app: vm-proxy-auth
    component: raft-cluster
spec:
  clusterIP: None
  selector:
    app: vm-proxy-auth
    component: raft-cluster
  ports:
  - name: http
    port: 8080
    targetPort: http
  - name: raft
    port: 9000
    targetPort: raft

---
apiVersion: v1
kind: Service
metadata:
  name: vm-proxy-auth-raft
  namespace: monitoring
  labels:
    app: vm-proxy-auth
    component: raft-cluster
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
spec:
  type: LoadBalancer
  selector:
    app: vm-proxy-auth
    component: raft-cluster
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vm-proxy-auth
  namespace: monitoring
  labels:
    app: vm-proxy-auth

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: vm-proxy-auth
  labels:
    app: vm-proxy-auth
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["update", "patch"]  # For self-registration annotations

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: vm-proxy-auth
  labels:
    app: vm-proxy-auth
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: vm-proxy-auth
subjects:
- kind: ServiceAccount
  name: vm-proxy-auth
  namespace: monitoring

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: vm-proxy-auth-raft-config
  namespace: monitoring
  labels:
    app: vm-proxy-auth
    component: configuration
data:
  config.yaml: |
    # VictoriaMetrics OAuth Gateway Configuration
    # Raft State Storage with Kubernetes Service Discovery Example

    server:
      address: ":8080"
      timeouts:
        readTimeout: 30s
        writeTimeout: 30s
        idleTimeout: 120s

    # Multiple upstream configuration with Raft state storage
    upstream:
      multiple:
        enabled: true
        
        # Backend configuration - will be discovered via Kubernetes
        backends:
          - url: "http://victoriametrics-1.monitoring.svc.cluster.local:8428"
            weight: 3
          - url: "http://victoriametrics-2.monitoring.svc.cluster.local:8428" 
            weight: 2
          - url: "http://victoriametrics-3.monitoring.svc.cluster.local:8428"
            weight: 1
            
        # Load balancing strategy
        loadBalancing:
          strategy: "least-connections"
          
        # Health checking configuration
        healthCheck:
          checkInterval: 30s
          timeout: 10s
          healthyThreshold: 2
          unhealthyThreshold: 3
          healthEndpoint: "/health"
          
        # Request queuing for high availability
        queue:
          enabled: true
          maxSize: 1000
          timeout: 5s
          
        # Request handling
        timeout: 30s
        maxRetries: 3
        retryBackoff: 100ms

    # Distributed state storage using Raft consensus
    stateStorage:
      type: "raft"
      raft:
        nodeId: "${RAFT_NODE_ID:-node-0}"  # Will be set by init container
        dataDir: "${RAFT_DATA_DIR:-/data/raft}"
        peers:
          - "node-0:vm-proxy-auth-raft-0.vm-proxy-auth-raft-headless.monitoring.svc.cluster.local:9000"
          - "node-1:vm-proxy-auth-raft-1.vm-proxy-auth-raft-headless.monitoring.svc.cluster.local:9000"
          - "node-2:vm-proxy-auth-raft-2.vm-proxy-auth-raft-headless.monitoring.svc.cluster.local:9000"

    # Service discovery configuration
    serviceDiscovery:
      type: "kubernetes"
      kubernetes:
        namespace: "monitoring"
        peerLabelSelector: "app=vm-proxy-auth,component=raft-cluster"
        backendLabelSelector: "app=victoriametrics"
        raftPortName: "raft"
        httpPortName: "http"
        updateInterval: 30s

    # Authentication configuration
    auth:
      jwt:
        # JWKS for production (recommended)
        jwks:
          url: "https://your-auth-provider.com/.well-known/jwks.json"
          cacheTTL: 1h
        
        # JWT validation settings
        algorithm: "RS256"
        issuer: "your-auth-provider"
        audience: "vm-proxy-auth"
        leeway: 30s

    # Tenant mapping configuration
    tenantMapping:
      - jwtClaim: "team_id"
        staticMapping:
          team-alpha: "1000"
          team-beta: "1001" 
          team-gamma: "1002"

    # Advanced tenant filtering
    tenantFilter:
      strategy: "orConditions"
      labels:
        accountLabel: "vm_account_id"
        projectLabel: "vm_project_id"
        useProjectId: true

    # Metrics and monitoring
    metrics:
      enabled: true
      path: "/metrics"
      
      # Raft state storage adds these metrics:
      # - vm_proxy_auth_raft_state{state} (Leader/Follower/Candidate)
      # - vm_proxy_auth_raft_term_total
      # - vm_proxy_auth_raft_commit_index
      # - vm_proxy_auth_raft_last_applied_index  
      # - vm_proxy_auth_raft_peer_count
      # - vm_proxy_auth_service_discovery_events_total{type, source}

    # Logging configuration
    logging:
      level: "info"
      format: "json"
      
      # Raft and service discovery structured logs include:
      # - raft_operation: leadership changes, log replication, snapshots
      # - cluster_state: current Raft state and leader information
      # - service_discovery: peer and backend discovery events
      # - peer_count: number of discovered Raft peers
      # - backend_count: number of discovered backend services